{
 "cells": [
  {
   "source": [
    "# Attr-Parameters\n",
    "This notebook shows how to use `DictDataClass` and `Parameters` to specify parametrisation. It comes with following advantages compared to other parametrisation schemes (eg. jsons)\n",
    "\n",
    "1.  No KeyErrors\n",
    "2.  Typed\n",
    "3.  Default values so only need to specify changes - (compared this to maintaining tons of json files for each configuration)\n",
    "4.  All attr features for free. Eg. validators, comparators etc.\n",
    "5.  Intellisense in ide\n",
    "6.  Both dict-like and dot access\n",
    "7.  Interconversion to/from dict as well as flattened dict\n",
    "8.  Easy De/serialisation from/to json and yaml.\n",
    "9. Direct instantiation of objects from parameters specifying constructor arguments.\n",
    "10.  Flexible + automatic disambiguation\n",
    "11. Easy hyper-parameter search\n",
    "12. And last but not the least, almost no overhead!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b0c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import attr\n",
    "from pprint import pprint as print\n",
    "from typing import Union, Optional\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from param_impl import DictDataClass, default_value, Settings, Parameters, InstantiationMixin"
   ]
  },
  {
   "source": [
    "The use of the various components of the framework can be summarized as follows:\n",
    "\n",
    "1.  `DictDataClass`: This forms the core of the framework. All parameter classes should at least subclass this (and additionally add the `@attr.s(auto_atribs=True)` decorator). All parameters should be specified as a hierarchy of classes subclassing it.\n",
    "2.  `default_value`: A function used specify default values when they are mutable eg. list, classes etc. Refer to [this](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments) to know why this is important.\n",
    "3.  `Settings`: A list-type class used to specify multiple values for a parameter for grid search. Supports all operations of a regular python `list`.\n",
    "4.  `Parameters`: This is a subclass of `DictDataClass` useful in case grid search is also desired. It adds `get_settings()` method that returns all combinations of parameters values specifed using `Settings`.  \n",
    "5.  `InstantiationMixin`: This Mixin can be added to a hirearchy of parameter classes that each specify constructor arguments of a particular class which in turn is specified by a `type` attribute. \n",
    "\n",
    "When using this parametrisation framework the following need to be followed:\n",
    "\n",
    "1.  Don't forget to add the decorator `@attr.s` and subclass `Parameters`.\n",
    "2.  For each attribute specify type and default value. If the type is a class, use `default_value()` function to specify it. \n",
    "3.  Some attributes may allow multiple types. In this case do the following:\n",
    "    1.  specify the type as `Union[type1, type2, ..., typek]`\n",
    "    2.  override the `@classmethod` called `get_disambiguators()` that returns a dictionary with all `Union` types in that dataclass as    \n",
    "    keys and a \"disambiguator\" functions as value. Disambiguator functions are functions that takes two inputs, an object and a union type\n",
    "    corresponding to all possible types that the object can have and returns the actual type of that object. To avoid repetition, specify \n",
    "    all disambiguators in the `disambiguate()` function.\n",
    "\n",
    "The following cell contains a sample parameters hierarchy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb75e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "def disambiguate(o, t): \n",
    "    lambdas = {\n",
    "        Union[AdamOptimizerParams, SGDOptimizerParams]: lambda o, _: SGDOptimizerParams if 'momentum' in o else AdamOptimizerParams,\n",
    "        Union[int, str]: lambda *_: None\n",
    "    }\n",
    "    if t in lambdas:\n",
    "        return lambdas[t](o, t)\n",
    "    # elif t == Union[t1, t2, t3]:  # Write disambiguator like this when a simple lambda is not possible\n",
    "    #     pass\n",
    "    else:\n",
    "        raise TypeError(\"Unknown Type\")\n",
    "\n",
    "class SimpleTagger:\n",
    "    def __init__(self, embedding_param=50, encoder=None):\n",
    "        self.embedding_param = embedding_param\n",
    "        self.encoder = encoder\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class EncoderParams(InstantiationMixin, Parameters):    # Note that mixins should come before the actual superclass.\n",
    "    type: str = 'torch.nn.LSTM'\n",
    "    hidden_size: int = 100\n",
    "    num_layers: int = 1\n",
    "    bias: bool = True\n",
    "    dropout: float = 0\n",
    "    bidirectional: bool = True\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class ModelParams(InstantiationMixin, Parameters):\n",
    "    type: str = '__main__.SimpleTagger'\n",
    "    embedding_param: Union[int, str] = 50\n",
    "    encoder: Optional[EncoderParams] = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_disambiguators(cls):\n",
    "        return {Union[int, str]: disambiguate}\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class AdamOptimizerParams(Parameters):\n",
    "    type: str = 'torch.optim.Adam'\n",
    "    lr: float = 0.001\n",
    "    dict_attr: typing.Dict = default_value({1: '1', 2: '2'})\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class SGDOptimizerParams(Parameters):\n",
    "    type: str = 'torch.optim.SGD'\n",
    "    lr: float = 0.001\n",
    "    momentum: float = 0.1\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class TrainingParams(Parameters):\n",
    "    num_epochs: int = 20\n",
    "    optimizer: Union[AdamOptimizerParams,\n",
    "                     SGDOptimizerParams] = default_value(AdamOptimizerParams())\n",
    "\n",
    "    @classmethod\n",
    "    def get_disambiguators(cls):\n",
    "        return {Union[AdamOptimizerParams, SGDOptimizerParams]: disambiguate}\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class TaggingParams(Parameters):\n",
    "    random_seed: int = 42\n",
    "    gpu_idx: int = -1\n",
    "    model: ModelParams = default_value(ModelParams())\n",
    "    training: TrainingParams = default_value(TrainingParams())\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        # this function is called by attr after __init__()\n",
    "        # useful to modify default values\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0f30e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = TaggingParams()"
   ]
  },
  {
   "source": [
    "## Attr Freebies\n",
    "All the features of attrs suchs as dunder methods, comparators, validattors etc. are available at your service!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='__main__.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001, dict_attr={1: '1', 2: '2'})))\nTrue\nFalse\n"
     ]
    }
   ],
   "source": [
    "# __repr__ method\n",
    "print(params)\n",
    "\n",
    "# equility comparison\n",
    "params1 = TaggingParams()\n",
    "params2 = TaggingParams(model=ModelParams(encoder=EncoderParams()))\n",
    "print(params == params1)\n",
    "print(params == params2)"
   ]
  },
  {
   "source": [
    "## Serialisation\n",
    "### Dicts\n",
    "Easy conversion to and from ensted dicts as well as flattened dicts. The latter is useful because many packages (eg. comet_ml) do not support nested configurations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3255e621",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': None,\n           'type': '__main__.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'dict_attr': {1: '1', 2: '2'},\n                            'lr': 0.001,\n                            'type': 'torch.optim.Adam'}}}\nTaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='__main__.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001, dict_attr={1: '1', 2: '2'})))\n"
     ]
    }
   ],
   "source": [
    "# easy conversion to and from dict\n",
    "print(params.to_dict())\n",
    "print(TaggingParams.from_dict(params.to_dict()))\n",
    "\n",
    "# the deserialized params are equal to original params\n",
    "assert TaggingParams.from_dict(params.to_dict()) == params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b9ba01",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder': None,\n 'model.type': '__main__.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.dict_attr': {1: '1', 2: '2'},\n 'training.optimizer.lr': 0.001,\n 'training.optimizer.type': 'torch.optim.Adam'}\n{'gpu_idx': -1,\n 'model_embedding_param': 50,\n 'model_encoder': None,\n 'model_type': '__main__.SimpleTagger',\n 'random_seed': 42,\n 'training_num_epochs': 20,\n 'training_optimizer_dict_attr': {1: '1', 2: '2'},\n 'training_optimizer_lr': 0.001,\n 'training_optimizer_type': 'torch.optim.Adam'}\nTaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='__main__.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001, dict_attr={1: '1', 2: '2'})))\n"
     ]
    }
   ],
   "source": [
    "# easy serialisation to flattend dict\n",
    "print(params.to_flattened_dict())\n",
    "\n",
    "# can use different spearator\n",
    "print(params.to_flattened_dict(sep='_')) \n",
    "\n",
    "# easy deserialisation from flattend dict\n",
    "print(TaggingParams.from_flattened_dict(params.to_flattened_dict()))\n",
    "\n",
    "# the deserialized params are equal to original params\n",
    "assert TaggingParams.from_flattened_dict(params.to_flattened_dict()) == params"
   ]
  },
  {
   "source": [
    "### JSON, YAML\n",
    "Helper methods to serialise to and deserialise from json and yaml."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='__main__.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001, dict_attr={1: '1', 2: '2'})))\nTaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='__main__.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001, dict_attr={'1': '1', '2': '2'})))\nFalse\n"
     ]
    }
   ],
   "source": [
    "# Easy serialisation to and deserialisation from json\n",
    "params.to_json(open('params.json', 'w'))\n",
    "_params = TaggingParams.from_json(open('params.json'))\n",
    "\n",
    "# Note however that params != _params because the keys in dict_attr have become string after deserialisation. This is a shortcoming of using json.\n",
    "print(params)\n",
    "print(_params)\n",
    "print(params == _params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy serialisation to and deserialisation from yaml\n",
    "params.to_yaml(open('params.yaml', 'w'))\n",
    "_params = TaggingParams.from_yaml(open('params.yaml'))\n",
    "assert params == _params"
   ]
  },
  {
   "source": [
    "## Flexible Attribute Access"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'embedding_param': 50, 'encoder': None, 'type': '__main__.SimpleTagger'}\n{'embedding_param': 50, 'encoder': None, 'type': '__main__.SimpleTagger'}\n"
     ]
    }
   ],
   "source": [
    "# Both dict-like and dot access are supported:\n",
    "print(params.model.to_dict())\n",
    "print(params['model'].to_dict())\n",
    "assert params.model == params['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 100,\n           'encoder': {'bias': True,\n                       'bidirectional': True,\n                       'dropout': 0,\n                       'hidden_size': 100,\n                       'num_layers': 1,\n                       'type': 'torch.nn.LSTM'},\n           'type': '__main__.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'dict_attr': {1: '1', 2: '2'},\n                            'lr': 0.001,\n                            'type': 'torch.optim.Adam'}}}\n{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': None,\n           'type': '__main__.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'dict_attr': {1: '1', 2: '2'},\n                            'lr': 0.001,\n                            'type': 'torch.optim.Adam'}}}\n"
     ]
    }
   ],
   "source": [
    "# can modify using both dict and attribute access\n",
    "_params = deepcopy(params)\n",
    "_params.model.encoder = EncoderParams()\n",
    "_params['model']['embedding_param'] = 100\n",
    "print(_params.to_dict())\n",
    "print(params.to_dict())"
   ]
  },
  {
   "source": [
    "## Direct Instantiation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'embedding_param': 50,\n",
      " 'encoder': {'bias': True,\n",
      "             'bidirectional': True,\n",
      "             'dropout': 0,\n",
      "             'hidden_size': 100,\n",
      "             'num_layers': 1,\n",
      "             'type': 'torch.nn.LSTM'},\n",
      " 'type': '__main__.SimpleTagger'}\n",
      "<__main__.SimpleTagger object at 0x7fae0899dfa0>\n",
      "LSTM(50, 100, bidirectional=True)\n"
     ]
    }
   ],
   "source": [
    "mp = ModelParams(encoder=EncoderParams())\n",
    "print(mp.to_dict())\n",
    "\n",
    "# This would produce error because EncoderParams does not specify input_size which is a required argument for torch.nn.LSTM\n",
    "# m = mp.instantiate() \n",
    "\n",
    "# This will work\n",
    "m = mp.instantiate(encoder={'input_size': 50})\n",
    "print(m)\n",
    "print(m.encoder)\n",
    "\n",
    "# If EncoderParams itself had some attributes that themselves can be instantiated and \n",
    "# do not specify all parameters then those also need to be passed as a nested dictionary. \n",
    "# The arguments to `instantiate()` can also be used to override parameter values."
   ]
  },
  {
   "source": [
    "## Hyper-parameter Search\n",
    "### Directly using `Parameters`\n",
    "`Parameters` can be directly used to specify the values to try out for each parameter and then to get all settings in the grid formed by product of values for each parameter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': {'bias': True,\n                       'bidirectional': True,\n                       'dropout': 0,\n                       'hidden_size': 100,\n                       'num_layers': 1,\n                       'type': 'torch.nn.LSTM'},\n           'type': '__main__.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'dict_attr': {1: '1', 2: '2'},\n                            'lr': 0.001,\n                            'type': 'torch.optim.Adam'}}}\n"
     ]
    }
   ],
   "source": [
    "params = TaggingParams(model=ModelParams(encoder=EncoderParams()))\n",
    "print(params.to_dict())"
   ]
  },
  {
   "source": [
    "Use `Settings` to specify different values for each parameter:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.model.encoder.hidden_size = Settings([50, 100])\n",
    "params.training.optimizer.lr = Settings([1e-2, 1e-1])"
   ]
  },
  {
   "source": [
    "Now just use the `get_settings()` function to get all the different possible settings:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 50,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': '__main__.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.dict_attr': {1: '1', 2: '2'},\n 'training.optimizer.lr': 0.01,\n 'training.optimizer.type': 'torch.optim.Adam'}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 50,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': '__main__.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.dict_attr': {1: '1', 2: '2'},\n 'training.optimizer.lr': 0.1,\n 'training.optimizer.type': 'torch.optim.Adam'}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 100,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': '__main__.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.dict_attr': {1: '1', 2: '2'},\n 'training.optimizer.lr': 0.01,\n 'training.optimizer.type': 'torch.optim.Adam'}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 100,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': '__main__.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.dict_attr': {1: '1', 2: '2'},\n 'training.optimizer.lr': 0.1,\n 'training.optimizer.type': 'torch.optim.Adam'}\n"
     ]
    }
   ],
   "source": [
    "settings = params.get_settings()\n",
    "print(len(settings))        # will be equal to the product of the number of values for each parameter\n",
    "for setting in settings:\n",
    "    print(setting.to_flattened_dict())"
   ]
  },
  {
   "source": [
    "It is also possible to do the above for attributes of list or any other more complex type."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TempParams(list_param=None)\n2\nTempParams(list_param=[1])\nTempParams(list_param=[1, 2])\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "@attr.s(auto_attribs=True)\n",
    "class TempParams(Parameters):\n",
    "    list_param: Optional[List[int]] = None\n",
    "p = TempParams()\n",
    "print(p)\n",
    "p.list_param = Settings([[1], [1,2]])\n",
    "s = p.get_settings()\n",
    "print(len(s))        # will be equal to the product of the number of values for each parameter\n",
    "for _s in s:\n",
    "    print(_s)"
   ]
  },
  {
   "source": [
    "### Using Raytune without Search Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/shivanshu/miniconda3/envs/dl/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ray import tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': {'bias': True,\n                       'bidirectional': True,\n                       'dropout': 0,\n                       'hidden_size': 100,\n                       'num_layers': 1,\n                       'type': 'torch.nn.LSTM'},\n           'type': '__main__.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'dict_attr': {1: '1', 2: '2'},\n                            'lr': [0.01, 0.1],\n                            'type': 'torch.optim.Adam'}}}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': {'grid_search': [50, 100]},\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': '__main__.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.dict_attr': {1: '1', 2: '2'},\n 'training.optimizer.lr': <ray.tune.sample.Float object at 0x7fae09561f70>,\n 'training.optimizer.type': 'torch.optim.Adam'}\n"
     ]
    }
   ],
   "source": [
    "params = TaggingParams(model=ModelParams(encoder=EncoderParams()))\n",
    "print(params.to_dict())\n",
    "params.model.encoder.hidden_size = tune.grid_search([50, 100])\n",
    "params.training.optimizer.lr = tune.loguniform(1e-3, 1e-1)\n",
    "print(params.to_flattened_dict())\n",
    "# Now just pass `params.to_flattened_dict()` as `config` parameter to `tune.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd00ae59ccd0a25b34c1138f46f30ecc7218c820f706af420d3d408be82c81b9c7c",
   "display_name": "Python 3.8.8 64-bit ('dl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "0ae59ccd0a25b34c1138f46f30ecc7218c820f706af420d3d408be82c81b9c7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}