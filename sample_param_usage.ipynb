{
 "cells": [
  {
   "source": [
    "# Attr-Parameters\n",
    "This notebook shows how to use `Parameters` which is a dataclass developed using the `attr` package along with all the additional features it provides."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b0c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import attr\n",
    "from pprint import pprint as print\n",
    "from typing import Union, Optional\n",
    "from copy import deepcopy"
   ]
  },
  {
   "source": [
    "The following need to be imported from `param_impl` module to get full benefit of this framework:\n",
    "\n",
    "1.  `Parameters`: This (data)class forms the core of the framework. All param classes should subclass this (and additionally add the `@attr.s(auto_atribs=True)` decorator).\n",
    "2.  `Settings`: A list-type class used to specify multiple values for a parameter for hyper-parameter search. Supports all operations of a regular python `list`.\n",
    "3.  `default_value`: A function used specify default values when they are mutable eg. list, class objects etc. Refer to [this](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments) to know why this is important."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from param_impl import Parameters, Settings, default_value"
   ]
  },
  {
   "source": [
    "Define the all the parameters as a hierarchy of dataclasses subclassing `Parameters`. Things to note:\n",
    "\n",
    "1.  Don't forget to add the decorator `@attr.s` and subclass `Parameters`.\n",
    "2.  For each attribute specify type and default value. If the type is a class, use `default_value()` function to specify it. \n",
    "3.  Some attributes may allow multiple types. In this case do the following:\n",
    "    1.  specify the type as `Union[type1, type2, ..., typek]`\n",
    "    2.  override the `@classmethod` called `get_disambiguators()` that returns a dictionary with all `Union` types in that dataclass as    \n",
    "    keys and a \"disambiguator\" functions as value. Disambiguator functions are functions that takes two inputs, an object and a union type\n",
    "    corresponding to all possible types that the object can have and returns the actual type of that object. To avoid repetition, specify \n",
    "    all disambiguators in the `disambiguate()` function.\n",
    "\n",
    "The following cell contains examples of the above process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb75e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate(o, t): \n",
    "    lambdas = {\n",
    "        Union[AdamOptimizerParams, SGDOptimizerParams]: lambda o, _: SGDOptimizerParams if 'momentum' in o else AdamOptimizerParams,\n",
    "        Union[int, str]: lambda *_: None\n",
    "    }\n",
    "    if t in lambdas:\n",
    "        return lambdas[t](o, t)\n",
    "    # elif t == Union[t1, t2, t3]:  # Write disambiguator like this when a simple lambda is not possible\n",
    "    #     pass\n",
    "    else:\n",
    "        raise TypeError(\"Unknown Type\")\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class EncoderParams(Parameters):\n",
    "    type: str = 'torch.nn.LSTM'\n",
    "    hidden_size: int = 100\n",
    "    num_layers: int = 1\n",
    "    bias: bool = True\n",
    "    dropout: float = 0\n",
    "    bidirectional: bool = True\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class ModelParams(Parameters):\n",
    "    type: str = 'models.simple_tagger.SimpleTagger'\n",
    "    embedding_param: Union[int, str] = 50\n",
    "    encoder: Optional[EncoderParams] = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_disambiguators(cls):\n",
    "        return {Union[int, str]: disambiguate}\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class AdamOptimizerParams(Parameters):\n",
    "    type: str = 'torch.optim.Adam'\n",
    "    lr: float = 0.001\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class SGDOptimizerParams(Parameters):\n",
    "    type: str = 'torch.optim.SGD'\n",
    "    lr: float = 0.001\n",
    "    momentum: float = 0.1\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class TrainingParams(Parameters):\n",
    "    num_epochs: int = 20\n",
    "    optimizer: Union[AdamOptimizerParams,\n",
    "                     SGDOptimizerParams] = default_value(AdamOptimizerParams())\n",
    "\n",
    "    @classmethod\n",
    "    def get_disambiguators(cls):\n",
    "        return {Union[AdamOptimizerParams, SGDOptimizerParams]: disambiguate}\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class TaggingParams(Parameters):\n",
    "    random_seed: int = 42\n",
    "    gpu_idx: int = -1\n",
    "    model: ModelParams = default_value(ModelParams())\n",
    "    training: TrainingParams = default_value(TrainingParams())\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        # this function is called by attr after __init__()\n",
    "        # useful to modify default values\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a0f30e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='models.simple_tagger.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001)))\n"
     ]
    }
   ],
   "source": [
    "params = TaggingParams()\n",
    "print(params)"
   ]
  },
  {
   "source": [
    "## Dictionary\n",
    "`Parameters` can be easily converted to and from dicts as well as flattened dicts. The latter is useful because many packages (eg. comet_ml) do not support nested configurations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3255e621",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': None,\n           'type': 'models.simple_tagger.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'lr': 0.001, 'type': 'torch.optim.Adam'}}}\nTaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='models.simple_tagger.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001)))\n"
     ]
    }
   ],
   "source": [
    "# easy conversion to and from dict\n",
    "print(params.to_dict())\n",
    "print(TaggingParams.from_dict(params.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48b9ba01",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder': None,\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': 0.001,\n 'training.optimizer.type': 'torch.optim.Adam'}\nTaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='models.simple_tagger.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001)))\n"
     ]
    }
   ],
   "source": [
    "# easy conversion to and from flattend dict\n",
    "print(params.to_flattened_dict())\n",
    "print(TaggingParams.from_flattened_dict(params.to_flattened_dict()))"
   ]
  },
  {
   "source": [
    "Equality comparison is supported out of the box (thanks to `attr`). So easy to check desirialising from dictionaries gives the same parameters:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "888da7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TaggingParams.from_dict(params.to_dict()) == params\n",
    "assert TaggingParams.from_flattened_dict(params.to_flattened_dict()) == params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'embedding_param': 50,\n 'encoder': None,\n 'type': 'models.simple_tagger.SimpleTagger'}\n{'embedding_param': 50,\n 'encoder': None,\n 'type': 'models.simple_tagger.SimpleTagger'}\n"
     ]
    }
   ],
   "source": [
    "# Both dict-like and attribute access are supported:\n",
    "print(params.model.to_dict())\n",
    "print(params['model'].to_dict())\n",
    "assert params.model == params['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 100,\n           'encoder': {'bias': True,\n                       'bidirectional': True,\n                       'dropout': 0,\n                       'hidden_size': 100,\n                       'num_layers': 1,\n                       'type': 'torch.nn.LSTM'},\n           'type': 'models.simple_tagger.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'lr': 0.001, 'type': 'torch.optim.Adam'}}}\n{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': None,\n           'type': 'models.simple_tagger.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'lr': 0.001, 'type': 'torch.optim.Adam'}}}\n"
     ]
    }
   ],
   "source": [
    "# can modify using both dict and attribute access\n",
    "_params = deepcopy(params)\n",
    "_params.model.encoder = EncoderParams()\n",
    "_params['model']['embedding_param'] = 100\n",
    "print(_params.to_dict())\n",
    "print(params.to_dict())"
   ]
  },
  {
   "source": [
    "## Hyper-parameter Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Directly using `Parameters`\n",
    "`Parameters` can be directly used to specify the values to try out for each parameter and then to get all settings in the grid formed by product of values for each parameter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': {'bias': True,\n                       'bidirectional': True,\n                       'dropout': 0,\n                       'hidden_size': 100,\n                       'num_layers': 1,\n                       'type': 'torch.nn.LSTM'},\n           'type': 'models.simple_tagger.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'lr': 0.001, 'type': 'torch.optim.Adam'}}}\n"
     ]
    }
   ],
   "source": [
    "params = TaggingParams(model=ModelParams(encoder=EncoderParams()))\n",
    "print(params.to_dict())"
   ]
  },
  {
   "source": [
    "Use `Settings` to specify different values for each parameter:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.model.encoder.hidden_size = Settings([50, 100])\n",
    "params.training.optimizer.lr = Settings([1e-2, 1e-1])"
   ]
  },
  {
   "source": [
    "Now just use the `get_settings()` function to get all the different possible settings:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 50,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': 0.01,\n 'training.optimizer.type': 'torch.optim.Adam'}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 50,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': 0.1,\n 'training.optimizer.type': 'torch.optim.Adam'}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 100,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': 0.01,\n 'training.optimizer.type': 'torch.optim.Adam'}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 100,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': 0.1,\n 'training.optimizer.type': 'torch.optim.Adam'}\n"
     ]
    }
   ],
   "source": [
    "settings = params.get_settings()\n",
    "print(len(settings))        # will be equal to the product of the number of values for each parameter\n",
    "for setting in settings:\n",
    "    print(setting.to_flattened_dict())"
   ]
  },
  {
   "source": [
    "It is also possible to do the above for attributes of list or any other more complex type."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TempParams(list_param=None)\n2\nTempParams(list_param=[1])\nTempParams(list_param=[1, 2])\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "@attr.s(auto_attribs=True)\n",
    "class TempParams(Parameters):\n",
    "    list_param: Optional[List[int]] = None\n",
    "p = TempParams()\n",
    "print(p)\n",
    "p.list_param = Settings([[1], [1,2]])\n",
    "s = p.get_settings()\n",
    "print(len(s))        # will be equal to the product of the number of values for each parameter\n",
    "for _s in s:\n",
    "    print(_s)"
   ]
  },
  {
   "source": [
    "### Using Raytune without Search Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': {'bias': True,\n                       'bidirectional': True,\n                       'dropout': 0,\n                       'hidden_size': 100,\n                       'num_layers': 1,\n                       'type': 'torch.nn.LSTM'},\n           'type': 'models.simple_tagger.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'lr': [0.01, 0.1], 'type': 'torch.optim.Adam'}}}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': {'grid_search': [50, 100]},\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': <ray.tune.sample.Float object at 0x7f9d9116c370>,\n 'training.optimizer.type': 'torch.optim.Adam'}\n"
     ]
    }
   ],
   "source": [
    "params = TaggingParams(model=ModelParams(encoder=EncoderParams()))\n",
    "print(params.to_dict())\n",
    "params.model.encoder.hidden_size = tune.grid_search([50, 100])\n",
    "params.training.optimizer.lr = tune.loguniform(1e-3, 1e-1)\n",
    "print(params.to_flattened_dict())\n",
    "# Now just pass `params.to_flattened_dict()` as `config` parameter to `tune.run()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd00ae59ccd0a25b34c1138f46f30ecc7218c820f706af420d3d408be82c81b9c7c",
   "display_name": "Python 3.8.8 64-bit ('dl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "0ae59ccd0a25b34c1138f46f30ecc7218c820f706af420d3d408be82c81b9c7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}